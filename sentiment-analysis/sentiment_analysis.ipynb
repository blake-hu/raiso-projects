{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* **Natural Language Processing (NLP):** The discipline of computer science, artificial intelligence and linguistics that is concerned with the creation of computational models that process and understand natural language. These include: making the computer understand the semantic grouping of words (e.g. cat and dog are semantically more similar than cat and spoon), text to speech, language translation and many more\n",
    "\n",
    "* **Sentiment Analysis:** It is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis allows organizations to identify public sentiment towards certain words or topics.\n",
    "\n",
    "In this notebook, we'll develop a **Sentiment Analysis model** to categorize a tweet as **Positive or Negative.**\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. [Importing dependencies](#p1)\n",
    "2. [Importing dataset](#p2)\n",
    "3. [Preprocessing Text](#p3)\n",
    "4. [Analysing data](#p4)\n",
    "5. [Splitting data](#p5)\n",
    "6. [TF-IDF Vectoriser](#p6)\n",
    "7. [Transforming Dataset](#p7)\n",
    "8. [Creating and Evaluating Models](#p8)\n",
    "9. [Saving the Models](#p9)\n",
    "10. [Using the Model](#p10)\n",
    "\n",
    "## <a name=\"p1\">Importing Dependencies</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p2\">Importing dataset</a>\n",
    "The dataset being used is the **sentiment140 dataset**. It contains 1,600,000 tweets extracted using the **Twitter API**. The tweets have been annotated **(0 = Negative, 4 = Positive)** and they can be used to detect sentiment.\n",
    " \n",
    "*[The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised.]*\n",
    "\n",
    "**It contains the following 6 fields:**\n",
    "1. **sentiment**: the polarity of the tweet *(0 = negative, 4 = positive)*\n",
    "2. **ids**: The id of the tweet *(2087)*\n",
    "3. **date**: the date of the tweet *(Sat May 16 23:58:44 UTC 2009)*\n",
    "4. **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "5. **user**: the user that tweeted *(robotickilldozr)*\n",
    "6. **text**: the text of the tweet *(Lyx is cool)*\n",
    "\n",
    "We require only the **sentiment** and **text** fields, so we discard the rest.\n",
    "\n",
    "Furthermore, we're changing the **sentiment** field so that it has new values to reflect the sentiment. **(0 = Negative, 1 = Positive)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('./reduced_dataset.csv',\n",
    "                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "\n",
    "# Removing the unnecessary columns.\n",
    "dataset = dataset[['sentiment','text']]\n",
    "# Replacing the values to ease understanding.\n",
    "dataset['sentiment'] = dataset['sentiment'].replace(4,1)\n",
    "\n",
    "# Plotting the distribution for dataset.\n",
    "ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n",
    "                                               legend=False)\n",
    "ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
    "\n",
    "# Storing data in lists.\n",
    "text, sentiment = list(dataset['text']), list(dataset['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p3\">Preprocess Text</a>\n",
    "**Text Preprocessing** is traditionally an important step for **Natural Language Processing (NLP)** tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n",
    "\n",
    "**The Preprocessing steps taken are:**\n",
    "1. **Lower Casing:** Each text is converted to lowercase.\n",
    "2. **Replacing URLs:** Links starting with **\"http\" or \"https\" or \"www\"** are replaced by **\"URL\"**.\n",
    "3. **Replacing Emojis:** Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. *(eg: \":)\" to \"EMOJIsmile\")*\n",
    "4. **Replacing Usernames:** Replace @Usernames with word **\"USER\"**. *(eg: \"@Kaggle\" to \"USER\")*\n",
    "5. **Removing Non-Alphabets:** Replacing characters except Digits and Alphabets with a space.\n",
    "6. **Removing Consecutive letters:** 3 or more consecutive letters are replaced by 2 letters. *(eg: \"Heyyyy\" to \"Heyy\")*\n",
    "7. **Removing Short Words:** Words with length less than 2 are removed.\n",
    "8. **Removing Stopwords:** Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. *(eg: \"the\", \"he\", \"have\")*\n",
    "9. **Lemmatizing:** Lemmatization is the process of converting a word to its base form. *(e.g: “Great” to “Good”)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "## Defining set containing all stopwords in english.\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    \n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Defining regex patterns.\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' URL',tweet)\n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
    "        # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' USER', tweet)        \n",
    "        # Replace all non alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            # Checking if the word is a stopword.\n",
    "            #if word not in stopwordlist:\n",
    "            if len(word)>1:\n",
    "                # Lemmatizing the word.\n",
    "                word = wordLemm.lemmatize(word)\n",
    "                tweetwords += (word+' ')\n",
    "            \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# Depending on your machine, this step may take a long time (up to a minute)\n",
    "print(\"Preprocessing...\")\n",
    "processedtext = preprocess(text)\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p5\">Exercise 1: Splitting the Data</a>\n",
    "\n",
    "Next, we will split the preprocessed data into training and test data. Do you remember what function we used for this?\n",
    "\n",
    "Divide **processedtext** (the \"X variable\") into: \n",
    "* **X_train:** the preprocessed text for **training** data\n",
    "* **X_test:** the preprocessed text for **test** data\n",
    "\n",
    "Divide **sentiment** (the \"y variable\") into:\n",
    "* **y_train:** the labelled sentiments for **training** data\n",
    "* **y_test:** the labelled sentiments for **test** data\n",
    "\n",
    "The test data should be 5% of your total data. You can specify this using a special keyword argument in your function (you'll want to read the function documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split done.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f'Data Split done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p6\">TF-IDF Vectoriser</a>\n",
    "**TF-IDF indicates what the importance of the word is in order to understand the document or dataset.** Let us understand with an example. Suppose you have a dataset where students write an essay on the topic, My House. In this dataset, the word a appears many times; it’s a high frequency word compared to other words in the dataset. The dataset contains other words like home, house, rooms and so on that appear less often, so their frequency are lower and they carry more information compared to the word. This is the intuition behind TF-IDF.\n",
    "\n",
    "**TF-IDF Vectoriser** converts a collection of raw documents to a **matrix of TF-IDF features**.\n",
    "\n",
    "1. Instantiate a TfidfVectorizer object with the following arguments:\n",
    "\n",
    "**ngram_range** is the range of number of words in a sequence. **Set ngram_range to (1,2).**\n",
    "<br>*[e.g \"very expensive\" is a 2-gram that is considered as an extra feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]* \n",
    "\n",
    "**max_features** specifies the number of features to consider. **Set max_features to 500000.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p7\">Bonus Exercise: Fit TF-IDF Vectoriser</a>\n",
    "\n",
    "1. Instantiate a TfidfVectorizer object with the following arguments:\n",
    "\n",
    "**ngram_range** is the range of number of words in a sequence. **Set ngram_range to (1,2).**\n",
    "<br>*[e.g \"very expensive\" is a 2-gram that is considered as an extra feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]* \n",
    "\n",
    "**max_features** specifies the number of features to consider. **Set max_features to 500000.**\n",
    "\n",
    "2. Fit your vectorizer object to X_train using a built-in method of the TfidfVectorizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting vectoriser...\n",
      "Vectoriser fitted.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vectoriser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVectoriser fitted.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo. of feature_words: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mvectoriser\u001b[49m\u001b[38;5;241m.\u001b[39mget_feature_names()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectoriser' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Fitting vectoriser...\")\n",
    "#YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f'Vectoriser fitted.')\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p7\">Exercise 2: Transforming the dataset</a>\n",
    "Transforming the **X_train** and **X_test** dataset into matrix of **TF-IDF Features** by using the **TF-IDF Vectoriser**. These datasets will be used to train the model and test against it.\n",
    "\n",
    "Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to find a built-in method of TF-IDF Vectoriser to transform these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data...\n",
      "Data Transformed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Transforming data...\")\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f'Data Transformed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p8\">Creating and Evaluating Models</a>\n",
    "\n",
    "We're using the Bernoulli Naive Bayes model for our sentiment analysis problem.\n",
    "\n",
    "Since our dataset is not **skewed**, i.e. it has equal number of **Positive and Negative** Predictions. We're choosing **Accuracy** as our evaluation metric. Furthermore, we're plotting the **Confusion Matrix** to get an understanding of how our model is performing on both classification types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "    \n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p8-1\">Exercise 3: Fitting BernoulliNB Model</a>\n",
    "We will be using the Bernoulli Naive Bayes classifier as our sentiment analysis model.\n",
    "\n",
    "Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) for sklearn's BernoulliNB model to:\n",
    "1. Instantiate a BernoulliNB object with parameter alpha=2 and call your model object BNBmodel.\n",
    "\n",
    "2. Fit your BNBmodel object on your training data.\n",
    "\n",
    "3. Evaluate your model using the model_Evaluate function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p8-1\">Exercise 4: Understanding Confusion Matrix</a>\n",
    "The model_Evaluate function produces a confusion matrix that provides data about your model's accuracy. Answer the following questions:\n",
    "\n",
    "1. What do each of the quadrants mean?\n",
    "2. What is the overall accuracy of your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p10\">Using the Model.</a>\n",
    "\n",
    "To use the model for **Sentiment Prediction** we need to import the **Vectoriser** and **LR Model** using **Pickle**.\n",
    "\n",
    "The vectoriser can be used to transform data to matrix of TF-IDF Features.\n",
    "While the model can be used to predict the sentiment of the transformed Data.\n",
    "The text whose sentiment has to be predicted however must be preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_sentiment(text):\n",
    "    # Text to classify should be in a list.\n",
    "    \n",
    "    textdata = vectoriser.transform(preprocess(text))\n",
    "    sentiment = BNBmodel.predict(textdata)\n",
    "    \n",
    "    # Make a list of text with sentiment.\n",
    "    data = []\n",
    "    for text, pred in zip(text, sentiment):\n",
    "        data.append((text,pred))\n",
    "        \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
    "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
    "\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter your text below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Text to classify should be in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectoriser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalyse_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melectron\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36manalyse_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyse_sentiment\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Text to classify should be in a list.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     textdata \u001b[38;5;241m=\u001b[39m \u001b[43mvectoriser\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(preprocess(text))\n\u001b[1;32m      5\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m BNBmodel\u001b[38;5;241m.\u001b[39mpredict(textdata)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Make a list of text with sentiment.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectoriser' is not defined"
     ]
    }
   ],
   "source": [
    "analyse_sentiment([\"electron\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ad703bc3a8c412f9c6ad67d2f94927f312da1fc2237b836535092dd63c42d7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
